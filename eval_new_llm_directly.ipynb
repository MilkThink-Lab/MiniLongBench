{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aeb0a714-9d15-41bc-ac06-ce5a1e448b1d",
   "metadata": {},
   "source": [
    "# Directly calculate the scores of LLMs on MiniLongBench\n",
    "\n",
    "In this notebook, we show how to obtain minilongbench socres directly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7008d96d-1ee5-44cf-91cb-293fb3e048bf",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7892164d-f5bb-4cef-9f4f-685a9af85679",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:228: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "from irt import *\n",
    "from utils import *\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97ed846c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/longbench.pickle', 'rb') as handle:\n",
    "    data = pickle.load(handle)\n",
    "scenarios_position, subscenarios_position = prepare_data(scenarios, data)\n",
    "Y = create_responses(scenarios, data)\n",
    "to_handle_scenario = 'longbench'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a0a2889",
   "metadata": {},
   "outputs": [],
   "source": [
    "balance_weights = np.ones(Y.shape[1])\n",
    "# per_scen indicates which scenario this document belongs to\n",
    "per_scen = [1, 1, 2, 1, 5, 3, 0, 0, 2, 1, 0, 4, 4, 4, 0, 2, 5, 3, 3, 3, 2]\n",
    "N = len(scenarios_position[to_handle_scenario])\n",
    "n_sub = len(scenarios[to_handle_scenario])\n",
    "for i, sub in enumerate(scenarios[to_handle_scenario]):\n",
    "    if per_scen[i] == 4:\n",
    "        num = 3\n",
    "    elif per_scen[i] == 5:\n",
    "        num = 2\n",
    "    else:\n",
    "        num = 4\n",
    "    n_i = len(subscenarios_position[to_handle_scenario][sub])\n",
    "    balance_weights[subscenarios_position[to_handle_scenario][sub]] = N/(num*6*n_i)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7717242",
   "metadata": {},
   "source": [
    "## calculate the scores of LLMs on the MiniLongBench benchmark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28f4bbcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llama_minilongbench_scores.pkl\n",
      "\n",
      "LongBench_narrativeqa                    0.03\n",
      "LongBench_qasper                         0.11\n",
      "LongBench_multifieldqa_en                0.23\n",
      "LongBench_multifieldqa_zh                0.12\n",
      "LongBench_hotpotqa                       0.07\n",
      "LongBench_2wikimqa                       0.08\n",
      "LongBench_musique                        0.15\n",
      "LongBench_dureader                       0.04\n",
      "LongBench_gov_report                     0.14\n",
      "LongBench_qmsum                          0.05\n",
      "LongBench_multi_news                     0.18\n",
      "LongBench_vcsum                          0.12\n",
      "LongBench_trec                           0.31\n",
      "LongBench_triviaqa                       0.31\n",
      "LongBench_samsum                         0.12\n",
      "LongBench_lsht                           0.25\n",
      "LongBench_passage_count                  0.0\n",
      "LongBench_passage_retrieval_en           0.07\n",
      "LongBench_passage_retrieval_zh           0.07\n",
      "LongBench_lcc                            0.71\n",
      "LongBench_repobench-p                    0.44\n",
      "\n",
      "Single-Document QA                       0.13\n",
      "Multi-Document QA                        0.09\n",
      "Summarization                            0.12\n",
      "Few-shot Learning                        0.25\n",
      "Synthetic Task                           0.05\n",
      "Code Completion                          0.57\n",
      "\n",
      "EN                                       0.19\n",
      "ZH                                       0.12\n",
      "ALL                                      0.20\n"
     ]
    }
   ],
   "source": [
    "def fun(arr, idx):\n",
    "    arr = np.array(arr)\n",
    "    res = np.empty_like(arr)\n",
    "    for i in range(len(arr)):\n",
    "        res[idx[i]] = arr[i]\n",
    "    return res\n",
    "\n",
    "to_handle_scenario = 'longbench'\n",
    "\n",
    "# Load the test samples of MiniLongBench\n",
    "with open(\"data/anchor.pkl\", \"rb\") as f:\n",
    "    anchor_points = pickle.load(f)\n",
    "    \n",
    "# minilongbench test cases\n",
    "seen_items = np.hstack([np.array(scenarios_position[scenario])[anchor_points[scenario]] for scenario in scenarios.keys()]).tolist()\n",
    "unseen_items = [i for i in range(4750) if i not in seen_items]\n",
    "\n",
    "name = scenarios[to_handle_scenario]\n",
    "\n",
    "# start indecates where the file begins\n",
    "start = [0, 13, 19, 31, 44, 70, 78, 85, 100, 111, 118, 124, 128, 143, 158, 167, 173, 196, 211, 219, 231, 237]\n",
    "# per_scen indicates which scenario this document belongs to\n",
    "per_scen = [1, 1, 2, 1, 5, 3, 0, 0, 2, 1, 0, 4, 4, 4, 0, 2, 5, 3, 3, 3, 2]\n",
    "# sort the sub-scenarios for easier result presentation\n",
    "idx = [5, 7, 8, 4, 19, 15, 2, 3, 10, 6, 0, 16, 17, 18, 1, 9, 20, 14, 12, 13, 11]\n",
    "\n",
    "\n",
    "# load the representations of the test samples\n",
    "all_pred_scores = []\n",
    "A, B, _ = load_irt_parameters('data/irt_model/')\n",
    "\n",
    "# evaluate all models in the evaluation folder\n",
    "for e, filename in enumerate(os.listdir('eval_data')):\n",
    "    print(filename)\n",
    "    \n",
    "    file_path = os.path.join('eval_data', filename)\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        minilongbench_scores = pickle.load(f)\n",
    "        \n",
    "    # scores for the 21 sub-scenarios\n",
    "    score_per_file = []\n",
    "    # scores for the 6 scenarios\n",
    "    score_per_scen = [0 for i in range(6)]\n",
    "    \n",
    "    # The direct test scores of MiniLongBench \n",
    "    minilongbench_scores = np.array(minilongbench_scores)\n",
    "    \n",
    "    # Display the scores\n",
    "    for i in range(21):\n",
    "        tmp_score = minilongbench_scores[start[i]:start[i+1]].mean()\n",
    "        score_per_file.append(tmp_score)\n",
    "        score_per_scen[per_scen[i]] += tmp_score\n",
    "    score_per_scen[0] /= 4\n",
    "    score_per_scen[1] /= 4\n",
    "    score_per_scen[2] /= 4\n",
    "    score_per_scen[3] /= 4\n",
    "    score_per_scen[4] /= 3\n",
    "    score_per_scen[5] /= 2\n",
    "    name = fun(name, idx)\n",
    "    score_per_file = fun(score_per_file, idx)\n",
    "    print()\n",
    "    \n",
    "    \n",
    "    for i in range(21):\n",
    "        print(f\"{name[i]:<40} {np.round(score_per_file[i], 2)}\")\n",
    "    print()\n",
    "    \n",
    "    print(\"{:<40} {:.2f}\".format(\"Single-Document QA\", np.round(score_per_scen[0], 2)))\n",
    "    print(\"{:<40} {:.2f}\".format(\"Multi-Document QA\", np.round(score_per_scen[1], 2)))\n",
    "    print(\"{:<40} {:.2f}\".format(\"Summarization\", np.round(score_per_scen[2], 2)))\n",
    "    print(\"{:<40} {:.2f}\".format(\"Few-shot Learning\", np.round(score_per_scen[3], 2)))\n",
    "    print(\"{:<40} {:.2f}\".format(\"Synthetic Task\", np.round(score_per_scen[4], 2)))\n",
    "    print(\"{:<40} {:.2f}\".format(\"Code Completion\", np.round(score_per_scen[5], 2)))\n",
    "\n",
    "    print()\n",
    "    en_idx = [0, 1, 2, 4, 5, 6, 8, 9, 10, 12, 13, 14, 16, 17, 19, 20]\n",
    "    zh_idx = [3, 7, 11, 15, 18]\n",
    "    en = np.mean(score_per_file[en_idx])\n",
    "    zh = np.mean(score_per_file[zh_idx])\n",
    "    print(\"{:<40} {:.2f}\".format(\"EN\", np.round(en, 2)))\n",
    "    print(\"{:<40} {:.2f}\".format(\"ZH\", np.round(zh, 2)))\n",
    "    print(\"{:<40} {:.2f}\".format(\"ALL\", np.round(np.mean(score_per_scen), 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c92d3d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
