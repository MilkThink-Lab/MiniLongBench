{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aeb0a714-9d15-41bc-ac06-ce5a1e448b1d",
   "metadata": {},
   "source": [
    "# Predict the scores of LLMs on the full LongBench benchmark\n",
    "\n",
    "In this notebook, we show how to obtain minilongbench socres by predicting the scores of LLMs on the full LongBench benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7008d96d-1ee5-44cf-91cb-293fb3e048bf",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7892164d-f5bb-4cef-9f4f-685a9af85679",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:228: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "from irt import *\n",
    "from utils import *\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97ed846c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/longbench.pickle', 'rb') as handle:\n",
    "    data = pickle.load(handle)\n",
    "scenarios_position, subscenarios_position = prepare_data(scenarios, data)\n",
    "Y = create_responses(scenarios, data)\n",
    "to_handle_scenario = 'longbench'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a0a2889",
   "metadata": {},
   "outputs": [],
   "source": [
    "balance_weights = np.ones(Y.shape[1])\n",
    "# per_scen indicates which scenario this document belongs to\n",
    "per_scen = [1, 1, 2, 1, 5, 3, 0, 0, 2, 1, 0, 4, 4, 4, 0, 2, 5, 3, 3, 3, 2]\n",
    "N = len(scenarios_position[to_handle_scenario])\n",
    "n_sub = len(scenarios[to_handle_scenario])\n",
    "for i, sub in enumerate(scenarios[to_handle_scenario]):\n",
    "    if per_scen[i] == 4:\n",
    "        num = 3\n",
    "    elif per_scen[i] == 5:\n",
    "        num = 2\n",
    "    else:\n",
    "        num = 4\n",
    "    n_i = len(subscenarios_position[to_handle_scenario][sub])\n",
    "    balance_weights[subscenarios_position[to_handle_scenario][sub]] = N/(num*6*n_i)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7717242",
   "metadata": {},
   "source": [
    "## predict the scores of LLMs on the full LongBench benchmark\n",
    "\n",
    "By using the previously constructed model, we predict the scores of new LLMs on Longbench, which will serve as the scores for minilongbench."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28f4bbcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llama_minilongbench_scores.pkl\n",
      "\n",
      "LongBench_narrativeqa                    0.03\n",
      "LongBench_qasper                         0.07\n",
      "LongBench_multifieldqa_en                0.17\n",
      "LongBench_multifieldqa_zh                0.08\n",
      "LongBench_hotpotqa                       0.06\n",
      "LongBench_2wikimqa                       0.05\n",
      "LongBench_musique                        0.02\n",
      "LongBench_dureader                       0.02\n",
      "LongBench_gov_report                     0.03\n",
      "LongBench_qmsum                          0.01\n",
      "LongBench_multi_news                     0.04\n",
      "LongBench_vcsum                          0.0\n",
      "LongBench_trec                           0.47\n",
      "LongBench_triviaqa                       0.71\n",
      "LongBench_samsum                         0.21\n",
      "LongBench_lsht                           0.15\n",
      "LongBench_passage_count                  0.0\n",
      "LongBench_passage_retrieval_en           0.04\n",
      "LongBench_passage_retrieval_zh           0.03\n",
      "LongBench_lcc                            0.64\n",
      "LongBench_repobench-p                    0.58\n",
      "\n",
      "Single-Document QA                       0.09\n",
      "Multi-Document QA                        0.04\n",
      "Summarization                            0.02\n",
      "Few-shot Learning                        0.39\n",
      "Synthetic Task                           0.02\n",
      "Code Completion                          0.61\n",
      "\n",
      "EN                                       0.20\n",
      "ZH                                       0.06\n",
      "ALL                                      0.20\n"
     ]
    }
   ],
   "source": [
    "def fun(arr, idx):\n",
    "    arr = np.array(arr)\n",
    "    res = np.empty_like(arr)\n",
    "    for i in range(len(arr)):\n",
    "        res[idx[i]] = arr[i]\n",
    "    return res\n",
    "\n",
    "to_handle_scenario = 'longbench'\n",
    "\n",
    "# Load the test samples of MiniLongBench\n",
    "with open(\"data/anchor.pkl\", \"rb\") as f:\n",
    "    anchor_points = pickle.load(f)\n",
    "    \n",
    "# minilongbench test cases\n",
    "seen_items = np.hstack([np.array(scenarios_position[scenario])[anchor_points[scenario]] for scenario in scenarios.keys()]).tolist()\n",
    "unseen_items = [i for i in range(4750) if i not in seen_items]\n",
    "\n",
    "name = scenarios[to_handle_scenario]\n",
    "\n",
    "# start indecates where the file begins\n",
    "start = [0, 200, 400, 600, 800, 1300, 1500, 1650, 1850, 2050, 2250, 2450, 2650, 2850, 3050, 3250, 3450, 3950, 4150, 4350, 4550, 4750]\n",
    "# per_scen indicates which scenario this document belongs to\n",
    "per_scen = [1, 1, 2, 1, 5, 3, 0, 0, 2, 1, 0, 4, 4, 4, 0, 2, 5, 3, 3, 3, 2]\n",
    "# sort the sub-scenarios for easier result presentation\n",
    "idx = [5, 7, 8, 4, 19, 15, 2, 3, 10, 6, 0, 16, 17, 18, 1, 9, 20, 14, 12, 13, 11]\n",
    "\n",
    "\n",
    "# load the representations of the test samples\n",
    "all_pred_scores = []\n",
    "A, B, _ = load_irt_parameters('data/irt_model/')\n",
    "\n",
    "# evaluate all models in the evaluation folder\n",
    "for e, filename in enumerate(os.listdir('eval_data')):\n",
    "    file_path = os.path.join('eval_data', filename)\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        minilongbench_scores = pickle.load(f)\n",
    "        \n",
    "    # scores for the 21 sub-scenarios\n",
    "    score_per_file = []\n",
    "    # scores for the 6 scenarios\n",
    "    score_per_scen = [0 for i in range(6)]\n",
    "    \n",
    "    # The direct test scores of MiniLongBench \n",
    "    minilongbench_scores = np.array(minilongbench_scores)\n",
    "    seen_items = sorted(seen_items)\n",
    "    \n",
    "    # Train the theta of the LLM\n",
    "    theta = estimate_ability_parameters(minilongbench_scores, A[:, :, seen_items], B[:, :, seen_items])\n",
    "    ind_seen = [u for u in seen_items if u in scenarios_position['longbench']]\n",
    "    ind_unseen = [u for u in unseen_items if u in scenarios_position['longbench']]\n",
    "    \n",
    "    # Use the representations of the test samples and the LLM to predict the LLM's scores on the entire Longbench\n",
    "    longbench_pred = np.zeros((4750))\n",
    "    longbench_pred[ind_seen] = minilongbench_scores\n",
    "    longbench_pred[ind_unseen] = (item_curve(theta, A, B))[0,ind_unseen]\n",
    "    pred_score = (balance_weights*longbench_pred).mean() # Pred performance\n",
    "    print(filename)\n",
    "    \n",
    "    \n",
    "    # Display the scores\n",
    "    for i in range(21):\n",
    "        tmp_score = longbench_pred[start[i]:start[i+1]].mean()\n",
    "        score_per_file.append(tmp_score)\n",
    "        score_per_scen[per_scen[i]] += tmp_score\n",
    "    score_per_scen[0] /= 4\n",
    "    score_per_scen[1] /= 4\n",
    "    score_per_scen[2] /= 4\n",
    "    score_per_scen[3] /= 4\n",
    "    score_per_scen[4] /= 3\n",
    "    score_per_scen[5] /= 2\n",
    "    name = fun(name, idx)\n",
    "    score_per_file = fun(score_per_file, idx)\n",
    "    print()\n",
    "    \n",
    "    \n",
    "    for i in range(21):\n",
    "        print(f\"{name[i]:<40} {np.round(score_per_file[i], 2)}\")\n",
    "    print()\n",
    "    \n",
    "    print(\"{:<40} {:.2f}\".format(\"Single-Document QA\", np.round(score_per_scen[0], 2)))\n",
    "    print(\"{:<40} {:.2f}\".format(\"Multi-Document QA\", np.round(score_per_scen[1], 2)))\n",
    "    print(\"{:<40} {:.2f}\".format(\"Summarization\", np.round(score_per_scen[2], 2)))\n",
    "    print(\"{:<40} {:.2f}\".format(\"Few-shot Learning\", np.round(score_per_scen[3], 2)))\n",
    "    print(\"{:<40} {:.2f}\".format(\"Synthetic Task\", np.round(score_per_scen[4], 2)))\n",
    "    print(\"{:<40} {:.2f}\".format(\"Code Completion\", np.round(score_per_scen[5], 2)))\n",
    "\n",
    "    print()\n",
    "    en_idx = [0, 1, 2, 4, 5, 6, 8, 9, 10, 12, 13, 14, 16, 17, 19, 20]\n",
    "    zh_idx = [3, 7, 11, 15, 18]\n",
    "    en = np.mean(score_per_file[en_idx])\n",
    "    zh = np.mean(score_per_file[zh_idx])\n",
    "    print(\"{:<40} {:.2f}\".format(\"EN\", np.round(en, 2)))\n",
    "    print(\"{:<40} {:.2f}\".format(\"ZH\", np.round(zh, 2)))\n",
    "    print(\"{:<40} {:.2f}\".format(\"ALL\", np.round(np.mean(score_per_scen), 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c346c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31488fe1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
