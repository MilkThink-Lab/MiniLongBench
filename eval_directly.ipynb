{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aeb0a714-9d15-41bc-ac06-ce5a1e448b1d",
   "metadata": {},
   "source": [
    "# Directly calculate the scores of LLMs on MiniLongBench\n",
    "\n",
    "In this notebook, we show how to obtain minilongbench socres directly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7008d96d-1ee5-44cf-91cb-293fb3e048bf",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7892164d-f5bb-4cef-9f4f-685a9af85679",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:228: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from irt import *\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26499fc1-2bda-44b2-9131-e78d16f7f77d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'longbench': ['LongBench_2wikimqa',\n",
       "  'LongBench_dureader',\n",
       "  'LongBench_gov_report',\n",
       "  'LongBench_hotpotqa',\n",
       "  'LongBench_lcc',\n",
       "  'LongBench_lsht',\n",
       "  'LongBench_multifieldqa_en',\n",
       "  'LongBench_multifieldqa_zh',\n",
       "  'LongBench_multi_news',\n",
       "  'LongBench_musique',\n",
       "  'LongBench_narrativeqa',\n",
       "  'LongBench_passage_count',\n",
       "  'LongBench_passage_retrieval_en',\n",
       "  'LongBench_passage_retrieval_zh',\n",
       "  'LongBench_qasper',\n",
       "  'LongBench_qmsum',\n",
       "  'LongBench_repobench-p',\n",
       "  'LongBench_samsum',\n",
       "  'LongBench_trec',\n",
       "  'LongBench_triviaqa',\n",
       "  'LongBench_vcsum']}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_handle_scenario = 'longbench'\n",
    "scenarios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34e5620-bd45-4985-b390-a154843b4d6c",
   "metadata": {},
   "source": [
    "Loading longbench test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ca68f5c-49de-4f75-92e5-de639059cec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/longbench.pickle', 'rb') as handle:\n",
    "    data = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee09c25b-2dc4-4403-a972-9fb05cfe917b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 4750)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scenarios_position, subscenarios_position = prepare_data(scenarios, data)\n",
    "Y = create_responses(scenarios, data)\n",
    "\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f40fc53-b11e-41cc-adc2-7abff1a2b368",
   "metadata": {},
   "outputs": [],
   "source": [
    "balance_weights = np.ones(Y.shape[1])\n",
    "# per_scen indicates which scenario this document belongs to\n",
    "per_scen = [1, 1, 2, 1, 5, 3, 0, 0, 2, 1, 0, 4, 4, 4, 0, 2, 5, 3, 3, 3, 2]\n",
    "N = len(scenarios_position[to_handle_scenario])\n",
    "n_sub = len(scenarios[to_handle_scenario])\n",
    "for i, sub in enumerate(scenarios[to_handle_scenario]):\n",
    "    if per_scen[i] == 4:\n",
    "        num = 3\n",
    "    elif per_scen[i] == 5:\n",
    "        num = 2\n",
    "    else:\n",
    "        num = 4\n",
    "    n_i = len(subscenarios_position[to_handle_scenario][sub])\n",
    "    balance_weights[subscenarios_position[to_handle_scenario][sub]] = N/(num*6*n_i)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844c4412-ae69-4184-b106-191a1c151736",
   "metadata": {},
   "source": [
    "## Split the data in train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc9874c5-7cb5-425b-8c41-9a87d7615ba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40, 4750) (20, 4750) (20, 4750)\n"
     ]
    }
   ],
   "source": [
    "train_idx = [32, 30, 25, 8, 0, 26, 7, 29, 12, 9, 23, 21, 1, 39, 6, 14, 11, 27, 20, 10]\n",
    "test_idx = [13, 34, 3, 38, 22, 24, 35, 37, 5, 4, 2, 19, 28, 33, 17, 18, 15, 36, 16, 31]\n",
    "\n",
    "Y_test = Y[test_idx]\n",
    "Y_train = Y[train_idx]\n",
    "Y_test_set = Y[test_idx]\n",
    "\n",
    "print(Y.shape, Y_train.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7717242",
   "metadata": {},
   "source": [
    "## calculate the scores of LLMs on the MiniLongBench benchmark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28f4bbcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The rank correlation between all LLMs on MiniLongBench and LongBench 0.9485928705440902\n",
      "The rank correlation between test LLMs on MiniLongBench and LongBench 0.9368421052631578\n",
      "The rank correlation between train LLMs on MiniLongBench and LongBench 0.9458646616541352\n",
      "[[0.9500938086303942, 0.9654135338345864, 0.9067669172932331], [0.9724202626641651, 0.9654135338345864, 0.9533834586466164], [0.9183864915572235, 0.9428571428571428, 0.8947368421052632], [0.9369606003752347, 0.9834586466165413, 0.8511278195488721], [0.95422138836773, 0.9548872180451128, 0.9278195488721804], [0.8418633099271289, 0.806015037593985, 0.8266266097929232]]\n"
     ]
    }
   ],
   "source": [
    "# number_item = Y_train.shape[1]\n",
    "from scipy.stats import spearmanr, kendalltau\n",
    "\n",
    "scenario_dict = {\"Single-Document QA\":[\"LongBench_narrativeqa\", \"LongBench_qasper\", \"LongBench_multifieldqa_en\", \"LongBench_multifieldqa_zh\"],\n",
    "                \"Multi-Document QA\":[\"LongBench_hotpotqa\", \"LongBench_2wikimqa\", \"LongBench_musique\", \"LongBench_dureader\"],\n",
    "                \"Summarization\":[\"LongBench_gov_report\", \"LongBench_qmsum\", \"LongBench_vcsum\", \"LongBench_samsum\"],\n",
    "                \"Few-shot Learning\":[\"LongBench_trec\", \"LongBench_lsht\", \"LongBench_triviaqa\", \"LongBench_multi_news\"],\n",
    "                \"Code Completion\":[\"LongBench_lcc\", \"LongBench_repobench-p\"],\n",
    "                \"Synthetic Task\":[\"LongBench_passage_count\", \"LongBench_passage_retrieval_en\", \"LongBench_passage_retrieval_zh\"]}\n",
    "\n",
    "\n",
    "A, B, _ = load_irt_parameters('data/irt_model/')\n",
    "X = np.vstack((A.squeeze(), B.squeeze().reshape((1,-1)))).T\n",
    "# X = np.vstack((A.squeeze())).T\n",
    "X = X[scenarios_position['longbench']]\n",
    "norm_balance_weights = balance_weights[scenarios_position['longbench']]\n",
    "norm_balance_weights /= norm_balance_weights.sum()\n",
    "scenario = 'longbench'\n",
    "with open('data/sub_scenarios_pospos.pkl', 'rb') as f:\n",
    "    sub_scenarios_pospos = pickle.load(f)\n",
    "    \n",
    "whole_sp = []\n",
    "test_sp = []\n",
    "train_sp = []\n",
    "whole_error = []\n",
    "test_error = []\n",
    "train_error = []\n",
    "whole_sub_sp = [[] for i in range(6)]\n",
    "test_sub_sp = [[] for i in range(6)]\n",
    "train_sub_sp = [[] for i in range(6)]\n",
    "whole_sub_score = [[] for i in range(6)]\n",
    "test_sub_score = [[] for i in range(6)]\n",
    "train_sub_score = [[] for i in range(6)]\n",
    "whole_sub_pred_score = [[] for i in range(6)]\n",
    "test_sub_pred_score = [[] for i in range(6)]\n",
    "train_sub_pred_score = [[] for i in range(6)]\n",
    "\n",
    "ratio = 0.95\n",
    "name = list(subscenarios_position['longbench'].keys())\n",
    "number_item = int((1-ratio) * 4750)\n",
    "\n",
    "corelation = []\n",
    "sub_sp = [[] for i in range(6)]\n",
    "\n",
    "for e in range(3):\n",
    "    # eval on all LLMs\n",
    "    if e == 0:\n",
    "        Y_test = Y\n",
    "    # eval on test LLMs\n",
    "    elif e == 1:\n",
    "        Y_test = Y_test_set\n",
    "    # eval on train LLMs\n",
    "    else:\n",
    "        Y_test = Y_train\n",
    "\n",
    "    # Load the test cases of MiniLongBench\n",
    "    with open(\"data/anchor.pkl\", \"rb\") as f:\n",
    "        anchor_points = pickle.load(f)\n",
    "\n",
    "    start = [0, 200, 400, 600, 800, 1300, 1500, 1650, 1850, 2050, 2250, 2450, 2650, 2850, 3050, 3250, 3450, 3950, 4150, 4350, 4550, 4750]\n",
    "    # per_scen indicates which scenario this document belongs to\n",
    "    per_scen = [1, 1, 2, 1, 4, 3, 0, 0, 2, 1, 0, 5, 5, 5, 0, 2, 4, 3, 3, 3, 2]\n",
    "    anchor_per_file = [[] for i in range(21)]\n",
    "    scores_per_file = [[] for i in range(21)]\n",
    "    anchor_scores_per_file = [[] for i in range(21)]\n",
    "    scores_per_scen = [np.zeros((Y_test.shape[0])) for i in range(6)]\n",
    "    anchor_scores_per_scen = [np.zeros((Y_test.shape[0])) for i in range(6)]\n",
    "    for item in sorted(anchor_points['longbench']):\n",
    "        for fi, s in enumerate(start[1:]):\n",
    "            if item < s:\n",
    "                anchor_per_file[fi].append(item)\n",
    "                break\n",
    "    summm = np.zeros((Y_test.shape[0]))\n",
    "\n",
    "    for i in range(21):\n",
    "        scores_per_file[i] = Y_test[:, subscenarios_position['longbench'][name[i]]].mean(axis=1)\n",
    "        anchor_scores_per_file[i] = Y_test[:, anchor_per_file[i]].mean(axis=1)\n",
    "        scores_per_scen[per_scen[i]] += Y_test[:, subscenarios_position['longbench'][name[i]]].mean(axis=1)\n",
    "        anchor_scores_per_scen[per_scen[i]] += Y_test[:, anchor_per_file[i]].mean(axis=1)\n",
    "        if i == 4 or i == 16:\n",
    "            summm += Y_test[:, anchor_per_file[i]].mean(axis=1) / 2 / 6\n",
    "        elif i == 11 or i == 12 or i == 13:\n",
    "            summm += Y_test[:, anchor_per_file[i]].mean(axis=1) / 3 / 6\n",
    "        else:\n",
    "            summm += Y_test[:, anchor_per_file[i]].mean(axis=1) / 4 / 6\n",
    "    \n",
    "\n",
    "    if e == 0:\n",
    "        whole_sub_score = scores_per_scen\n",
    "        whole_sub_pred_score = anchor_scores_per_scen\n",
    "    elif e == 1:\n",
    "        test_sub_score = scores_per_scen\n",
    "        test_sub_pred_score = anchor_scores_per_scen\n",
    "    else:\n",
    "        train_sub_score = scores_per_scen\n",
    "        train_sub_pred_score = anchor_scores_per_scen\n",
    "\n",
    "    \n",
    "    Y_true = (balance_weights*Y_test)[:,scenarios_position[scenario]].mean(axis=1)\n",
    "    spearman_corr, spearman_p_value = spearmanr(summm, Y_true)\n",
    "    corelation.append(spearman_corr)\n",
    "    for i in range(6):\n",
    "        spearman_corr, spearman_p_value = spearmanr(scores_per_scen[i], anchor_scores_per_scen[i])\n",
    "        sub_sp[i].append(spearman_corr)\n",
    "\n",
    "print(\"The rank correlation between all LLMs on MiniLongBench and LongBench\", corelation[0])\n",
    "print(\"The rank correlation between test LLMs on MiniLongBench and LongBench\", corelation[1])\n",
    "print(\"The rank correlation between train LLMs on MiniLongBench and LongBench\", corelation[2])\n",
    "print(sub_sp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52365c02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
