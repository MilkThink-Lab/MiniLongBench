# <img src="figure/logo.png" width="40" height="40"> [ACL 25] MiniLongBench: The Low-cost Long Context Understanding Benchmark for Large Language Models

[![Project Status](https://img.shields.io/badge/status-active-brightgreen.svg)]()
[![Python 3.11+](https://img.shields.io/badge/python-3.11%2B-blue)]()
![GitHub](https://img.shields.io/github/license/gbup-group/DIANet.svg)
[![paper](https://img.shields.io/badge/cs.AI-2503.10657-b31b1b?logo=arxiv&logoColor=red)](https://arxiv.org/abs/2503.10657)


This repository is the official codebase of our paper "MiniLongBench: The Low-cost Long Context Understanding Benchmark for Large Language Models" 

The proposed MiniLongBench is a low-cost benchmark for evaluating the Long Context Understanding (LCU) capabilities of LLMs, featuring a compact yet diverse test set of only **237 samples** spanning 6 major task categories and 21 distinct tasks.

Through empirical analysis of over 60 LLMs, MiniLongBench reduces the average evaluation cost to only 4.5% of the original while maintaining an average rank correlation coefficient of 0.97 with LongBench results.


## üéâ News

2025-05 - We released MiniLongBench dataset in [[Baidu Drive]](https://pan.baidu.com/s/1vUq3C5JR3ICo_g8_JXxJ0w?pwd=6erx) [[Google Drive]](https://drive.google.com/drive/folders/1Ps1_VoI1ExI1ZvVbBSCEKBuJGUlTeMmA?usp=sharing) [[Hugging Face]](https://huggingface.co/datasets/linggm/MiniLongBench). üëàüéâPlease try it! 

2025-05 - Our paper "MiniLongBench" has been accepted to the main track of ACL! [[Paper]](https://huggingface.co/datasets/linggm/MiniLongBench)üëàüéâPlease read it! 


## ‚öôÔ∏è Environment Setup
Create a Python virtual environment and install all the packages listed in the ```requirements.txt```.
```bash
conda create -n MiniLongBench python=3.11
conda activate MiniLongBench
pip install -r requirements.txt
```

To reproduce the construction of MiniLongBench, please install an apapted version of [[py-irt]](https://github.com/linggm3/py-irt).
```bash
pip install poetry
git clone https://github.com/linggm3/py-irt.git
cd py-irt
poetry install
```


## üß™ Testing on MiniLongBench
### Obtain LLM's output on MiniLongBench
Download MiniLongBench  [[Baidu Drive]](https://pan.baidu.com/s/1vUq3C5JR3ICo_g8_JXxJ0w?pwd=6erx) [[Google Drive]](https://drive.google.com/drive/folders/1Ps1_VoI1ExI1ZvVbBSCEKBuJGUlTeMmA?usp=sharing) [[Hugging Face]](https://huggingface.co/datasets/linggm/MiniLongBench)

Obtain LLM responses on OpenCompass:

* Evaluate the LLM across all 237 test samples in MiniLongBench
* Generate outputs in the format: pred_data/example


### Calculate scores across the all test samples.

To generate and store the evaluation scores on 237 test samples:

```bash
python minilongbench_scorer.py
```


### Calculate scores on MiniLongBench 
There are two evaluation methods for MiniLongBench.

1. Predict the scores of LLMs on the full LongBench benchmark (`eval_new_llm_by_pred.ipynb`): This notebook show how to obtain MiniLongBench socres by predicting the scores of LLMs on the full LongBench benchmark.

2. Directly calculate the scores of LLMs on MiniLongBench (`eval_new_llm_directly.ipynb`): This notebook show how to obtain MiniLongBench socres directly.



## üõ†Ô∏è Reproducing the MiniLongBench 
### Representation Learning
`representation_learning.ipynb` demonstrates how to load LongBench's evaluation data, perform data preprocessing, and learn representations for both the LLMs and test samples.

### Sample Clustering
`sample_clustering.ipynb` demonstrates how to cluster the representations of test samples and extract cluster centers as representative test samples. 

### Evaluation
There are two evaluation methods for MiniLongBench.

1. Predict the scores of LLMs on the full LongBench benchmark (`eval_by_pred.ipynb`).
2. Directly calculate the scores of LLMs on MiniLongBench (`eval_directly.ipynb`).


## üìú Citation  

```bibtex
@inproceedings{xxx,
  title={xxx},
  author={xxx},
  booktitle={xxx},
  year={xxx}
}
```
